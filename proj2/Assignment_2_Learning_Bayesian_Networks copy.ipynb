{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gmprovan/CS4705/blob/main/Assignment_2_Learning_Bayesian_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISw4_Z_R1d_8"
   },
   "source": [
    "The purpose of this notebook is to gain some practice in learning graphical models. Your goal is to:\n",
    "\n",
    "1.   load the Breast Cancer (categorical) data set: https://archive.ics.uci.edu/ml/datasets/breast+cancer\n",
    "2.   keep the last 20% of the data for testing\n",
    "3.   compare the performance of 3 learned models on the test data: naive Bayes, tree-structured BN (using the Chow-Liu algorithm), and BN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anido\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pgmpy.estimators import K2Score, BicScore\n",
    "from pgmpy.models import BayesianNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('./data/breast-cancer.data',\n",
    "                   names=['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change ? to NaN\n",
    "data[data == \"?\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"age\"] != \"20-29\"]\n",
    "data = data[data[\"inv-nodes\"] != \"24-25\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate train and test data\n",
    "dtrain, dtest = train_test_split(data.fillna(data.mode().iloc[0]), test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE3ITHto7e5v"
   },
   "source": [
    "## Naive Bayes model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the naive bayes I use the maximum likelihood estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb = BayesianNetwork(model_nb.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NhVCpGKUM3q3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+----------+\n",
      "| Class(no-recurrence-events) | 0.688596 |\n",
      "+-----------------------------+----------+\n",
      "| Class(recurrence-events)    | 0.311404 |\n",
      "+-----------------------------+----------+\n",
      "test:\n",
      " K2: -613.0600117480079 | Bic: -655.5336891240778\n",
      "train:\n",
      " K2: -2331.4476306110555 | Bic: -2380.6982978994574\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.models.NaiveBayes import NaiveBayes\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "# create the structure manually to create model_struct\n",
    "\n",
    "#model_nb = NaiveBayes(data.columns[1:], data.columns[0])\n",
    "\n",
    "model_nb.fit(dtrain, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "print(model_nb.get_cpds('Class'))\n",
    "\n",
    "# print the score\n",
    "print(\"test:\\n\", \"K2:\", K2Score(dtest).score(model_nb), \"| Bic:\", BicScore(dtest).score(model_nb))\n",
    "print(\"train:\\n\", \"K2:\", K2Score(dtrain).score(model_nb), \"| Bic:\", BicScore(dtrain).score(model_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 1527.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7017543859649122"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model_nb.predict(dtest[data.columns[1:]]).values == dtest[[\"Class\"]].values).sum()/len(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores obtained are good, we do seem to have a big difference between test and training data, which means that this model is not a good representation for general data.\n",
    "\n",
    "I decide to check the predictive accuracy independently, because I do not quite understand the previous scores.\n",
    "Since I get an error trying to use the .predict for naive bayes, I have implemented the predictions generation for this specific case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.6140350877192983\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for row in dtest.values:\n",
    "    result = [1, 1]\n",
    "    # test an example\n",
    "    for j, d in enumerate(model_nb.cpds[0].values):\n",
    "        for i, val in enumerate(row[1:]):\n",
    "            if model_nb.get_cpds(data.columns[i+1]).name_to_no[data.columns[i+1]].get(val, None) != None:\n",
    "                result[j] *= float(model_nb.get_cpds(data.columns[i+1]).values[model_nb.get_cpds(data.columns[i+1]).name_to_no[data.columns[i+1]][val]][j])\n",
    "            else: \n",
    "                result[j] *= 0.000001\n",
    "\n",
    "    predictions += [model_nb.cpds[0].state_names[\"Class\"][result.index(max(result))]]\n",
    "print(\"Test accuracy: \", (predictions == dtest.Class.values).sum()/len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.6140350877192983\n",
      "Train accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for row in dtest.values:\n",
    "    result = [1, 1]\n",
    "    # test an example\n",
    "    for j, d in enumerate(model_nb.cpds[0].values):\n",
    "        for i, val in enumerate(row[1:]):\n",
    "            if model_nb.get_cpds(data.columns[i+1]).name_to_no[data.columns[i+1]].get(val, None) != None:\n",
    "                result[j] *= float(model_nb.get_cpds(data.columns[i+1]).values[model_nb.get_cpds(data.columns[i+1]).name_to_no[data.columns[i+1]][val]][j])\n",
    "            else: \n",
    "                result[j] *= 0.000001\n",
    "\n",
    "    predictions += [model_nb.cpds[0].state_names[\"Class\"][result.index(max(result))]]\n",
    "print(\"Test accuracy: \", (predictions == dtest.Class.values).sum()/len(predictions))\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for row in dtrain.values:\n",
    "    result = [1, 1]\n",
    "    # test an example\n",
    "    for j, d in enumerate(model_nb.cpds[0].values):\n",
    "        for i, val in enumerate(row[1:]):\n",
    "            if model_nb.get_cpds(data.columns[i+1]).name_to_no[data.columns[i+1]].get(val, None) != None:\n",
    "                result[j] *= float(model_nb.get_cpds(data.columns[i+1]).values[model_nb.get_cpds(data.columns[i+1]).name_to_no[data.columns[i+1]][val]][j])\n",
    "            else: \n",
    "                result[j] *= 0.000001\n",
    "\n",
    "    predictions += [model_nb.cpds[0].state_names[\"Class\"][result.index(max(result))]]\n",
    "print(\"Train accuracy: \", (predictions == dtrain.Class.values).sum()/len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing these results, the model seems to work with somewhat acceptable performance, we do see that there is a `10%` difference between train and test accuracy, which is consistent with the previous scores. We can probably get a better model using more complex networks, so I expect the following models to perform better than this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0ADjdH07jFb"
   },
   "source": [
    "## Tree-structured model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2W52lkXeMqqr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building tree: 100%|██████████| 45/45.0 [00:00<00:00, 1260.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import TreeSearch\n",
    "\n",
    "# Impute missing data with the mode\n",
    "dtrain = dtrain.fillna(dtrain.mode().iloc[0])\n",
    "\n",
    "# learn graph structure using TreeSearch and Chow-Liu algorithm\n",
    "est = TreeSearch(dtrain, root_node=\"Class\")\n",
    "dag = est.estimate(estimator_type=\"chow-liu\")\n",
    "\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "\n",
    "# there are many choices of parametrization, here is one example\n",
    "model = BayesianNetwork(dag.edges())\n",
    "model.fit(\n",
    "    data, estimator=BayesianEstimator, prior_type=\"dirichlet\", pseudo_counts=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:\n",
      " K2: -602.3680239908574 | Bic: -800.6569955287398\n",
      "train:\n",
      " K2: -2236.4249916106974 | Bic: -2546.7317696896125\n"
     ]
    }
   ],
   "source": [
    "# print the score\n",
    "print(\"test:\\n\", \"K2:\", K2Score(dtest).score(model), \"| Bic:\", BicScore(dtest).score(model))\n",
    "print(\"train:\\n\", \"K2:\", K2Score(dtrain).score(model), \"| Bic:\", BicScore(dtrain).score(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 1047.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  0.7543859649122807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [00:00<00:00, 925.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  0.7105263157894737\n"
     ]
    }
   ],
   "source": [
    "# print the accuracy\n",
    "print(\"test: \", (model.predict(dtest[data.columns[1:]]).values == dtest[[data.columns[0]]].reset_index(drop=True).values).sum() / len(dtest))\n",
    "print(\"train: \", (model.predict(dtrain[data.columns[1:]]).values == dtrain[[data.columns[0]]].reset_index(drop=True).values).sum() / len(dtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the scores of the two models, we can see that the Naive Bayes model has a lower score for all the values. This model seems to be good, specially since we get a very similar accuracy for the train and test set, which seems to indicate that the model is a good generalization and does not overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionallly we can see the predictive accuracy for the other variables, since it is a Bayesian Model, it is easy to predict the value for any missing variable from all the others using the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 2239.14it/s]\n",
      "100%|██████████| 215/215 [00:00<00:00, 954.56it/s]\n",
      "100%|██████████| 56/56 [00:00<00:00, 1139.70it/s]\n",
      "100%|██████████| 206/206 [00:00<00:00, 1019.88it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 1255.05it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1012.05it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 1635.99it/s]\n",
      "100%|██████████| 183/183 [00:00<00:00, 934.44it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 1678.75it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 760.17it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 763.73it/s]\n",
      "100%|██████████| 217/217 [00:00<00:00, 677.71it/s]\n",
      "100%|██████████| 56/56 [00:00<00:00, 1366.55it/s]\n",
      "100%|██████████| 211/211 [00:00<00:00, 787.43it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 885.20it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 757.43it/s]\n",
      "100%|██████████| 56/56 [00:00<00:00, 3327.59it/s]\n",
      "100%|██████████| 198/198 [00:00<00:00, 778.93it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 947.04it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 498.92it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = {}\n",
    "for col in data:\n",
    "    preds[col] = [((model.predict(dtest.drop(columns=[col])).values == dtest[[col]].reset_index(drop=True).values).sum() / len(dtest)),\n",
    "                  ((model.predict(dtrain.drop(columns=[col])).values == dtrain[[col]].reset_index(drop=True).values).sum() / len(dtrain))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test | train\n",
      "Class [0.7543859649122807, 0.7105263157894737]\n",
      "age [0.5263157894736842, 0.5131578947368421]\n",
      "menopause [0.8947368421052632, 0.8114035087719298]\n",
      "tumor-size [0.24561403508771928, 0.2850877192982456]\n",
      "inv-nodes [0.8070175438596491, 0.8070175438596491]\n",
      "node-caps [0.8421052631578947, 0.8859649122807017]\n",
      "deg-malig [0.543859649122807, 0.5131578947368421]\n",
      "breast [0.6491228070175439, 0.6754385964912281]\n",
      "breast-quad [0.45614035087719296, 0.5263157894736842]\n",
      "irradiat [0.8245614035087719, 0.7850877192982456]\n"
     ]
    }
   ],
   "source": [
    "print(\"test\", \"|\", \"train\")\n",
    "for col in preds:\n",
    "    print(col, preds[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some variables that are very bad in this case, like `tumor-size`, `deg-malign`, `breast-quad` and `age`, whereas others are even better than the `Class` variable, it is very difficult to obtain a good score for all the variables but in the same way it is good to know we have some way to predict other variables if we have any problems.\n",
    "\n",
    "This can also help us understand which variables are actually important to the model, we could continue trying some different subset of the variables to try to obtain a better model.\n",
    "\n",
    "The following approach takes this into account and in the structure building it may be possible that some variables are not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Network model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn Structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model creation I use hill climbing, because the whole searching space is very big this approach will get to a local minimum, if I had more time I might have tried initializing with different random structures or include some kind of randomness in the search to try to get to a better solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('menopause', 'age'), ('node-caps', 'inv-nodes'), ('node-caps', 'deg-malig'), ('deg-malig', 'Class'), ('breast', 'breast-quad'), ('irradiat', 'node-caps')]\n",
      "test:\n",
      " K2: -455.10833332994423 | Bic: -479.57666873007065\n",
      "train:\n",
      " K2: -1701.8622890331535 | Bic: -1729.0220587763633\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import HillClimbSearch, BicScore, K2Score, BDeuScore\n",
    "\n",
    "est = HillClimbSearch(dtrain)\n",
    "\n",
    "# estimate the DAG\n",
    "model = BayesianNetwork(est.estimate(scoring_method=BicScore(dtrain), show_progress=False).edges()) \n",
    "print(model.edges())\n",
    "\n",
    "model.fit(dtrain, estimator=BayesianEstimator, prior_type=\"dirichlet\", pseudo_counts=0.1)\n",
    "\n",
    "# print the score\n",
    "print(\"test:\\n\", \"K2:\", K2Score(dtest).score(model), \"| Bic:\", BicScore(dtest).score(model))\n",
    "print(\"train:\\n\", \"K2:\", K2Score(dtrain).score(model), \"| Bic:\", BicScore(dtrain).score(model))\n",
    "\n",
    "#print((model.predict(dtest[list(model.nodes()._nodes.keys())].drop(columns=[\"Class\"])).values == dtest[[data.columns[0]]].reset_index(drop=True).values).sum() / len(dtest))\n",
    "#print((model.predict(dtrain[list(model.nodes()._nodes.keys())].drop(columns=[\"Class\"])).values == dtrain[[data.columns[0]]].reset_index(drop=True).values).sum() / len(dtrain))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are the worst among all the models, this may be because the model gets stuck in a local minimum and stops. That is why I would try to initialize the model in a different way to try to avoid getting stuck in this place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same as before I have not been able to use the prediction tools in the library so I created one that works for this model so I can compare the accuracy for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He score and predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>age</th>\n",
       "      <th>menopause</th>\n",
       "      <th>tumor-size</th>\n",
       "      <th>inv-nodes</th>\n",
       "      <th>node-caps</th>\n",
       "      <th>deg-malig</th>\n",
       "      <th>breast</th>\n",
       "      <th>breast-quad</th>\n",
       "      <th>irradiat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>recurrence-events</td>\n",
       "      <td>50-59</td>\n",
       "      <td>ge40</td>\n",
       "      <td>30-34</td>\n",
       "      <td>6-8</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>left</td>\n",
       "      <td>right_low</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>no-recurrence-events</td>\n",
       "      <td>40-49</td>\n",
       "      <td>premeno</td>\n",
       "      <td>10-14</td>\n",
       "      <td>0-2</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>left</td>\n",
       "      <td>left_low</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>no-recurrence-events</td>\n",
       "      <td>40-49</td>\n",
       "      <td>premeno</td>\n",
       "      <td>25-29</td>\n",
       "      <td>0-2</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>right</td>\n",
       "      <td>left_up</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>recurrence-events</td>\n",
       "      <td>50-59</td>\n",
       "      <td>ge40</td>\n",
       "      <td>30-34</td>\n",
       "      <td>3-5</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>left</td>\n",
       "      <td>left_low</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>no-recurrence-events</td>\n",
       "      <td>30-39</td>\n",
       "      <td>premeno</td>\n",
       "      <td>15-19</td>\n",
       "      <td>0-2</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>left</td>\n",
       "      <td>left_low</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Class    age menopause tumor-size inv-nodes node-caps  \\\n",
       "270     recurrence-events  50-59      ge40      30-34       6-8       yes   \n",
       "38   no-recurrence-events  40-49   premeno      10-14       0-2        no   \n",
       "160  no-recurrence-events  40-49   premeno      25-29       0-2        no   \n",
       "285     recurrence-events  50-59      ge40      30-34       3-5        no   \n",
       "171  no-recurrence-events  30-39   premeno      15-19       0-2        no   \n",
       "\n",
       "     deg-malig breast breast-quad irradiat  \n",
       "270          3   left   right_low       no  \n",
       "38           2   left    left_low       no  \n",
       "160          3  right     left_up      yes  \n",
       "285          3   left    left_low       no  \n",
       "171          1   left    left_low       no  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\anido\\Documents\\cml projs n stuff\\proj2\\Assignment_2_Learning_Bayesian_Networks copy.ipynb Cell 32\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/anido/Documents/cml%20projs%20n%20stuff/proj2/Assignment_2_Learning_Bayesian_Networks%20copy.ipynb#Y454sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m j, d \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(model\u001b[39m.\u001b[39mcpds[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mvalues):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/anido/Documents/cml%20projs%20n%20stuff/proj2/Assignment_2_Learning_Bayesian_Networks%20copy.ipynb#Y454sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mget_cpds(\u001b[39m\"\u001b[39m\u001b[39mdeg-malig\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mname_to_no[\u001b[39m\"\u001b[39m\u001b[39mdeg-malig\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(row[\u001b[39m6\u001b[39m], \u001b[39mNone\u001b[39;00m) \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/anido/Documents/cml%20projs%20n%20stuff/proj2/Assignment_2_Learning_Bayesian_Networks%20copy.ipynb#Y454sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         result[j] \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(model\u001b[39m.\u001b[39mget_cpds(\u001b[39m\"\u001b[39m\u001b[39mdeg-malig\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mvalues[model\u001b[39m.\u001b[39mget_cpds(\u001b[39m\"\u001b[39m\u001b[39mdeg-malig\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mname_to_no[\u001b[39m\"\u001b[39m\u001b[39mdeg-malig\u001b[39m\u001b[39m\"\u001b[39m][row[\u001b[39m6\u001b[39m]]][j])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/anido/Documents/cml%20projs%20n%20stuff/proj2/Assignment_2_Learning_Bayesian_Networks%20copy.ipynb#Y454sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39melse\u001b[39;00m: \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/anido/Documents/cml%20projs%20n%20stuff/proj2/Assignment_2_Learning_Bayesian_Networks%20copy.ipynb#Y454sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         result[j] \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.000001\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for row in dtest.values:\n",
    "    result = [1, 1]\n",
    "    # test an example\n",
    "    for j, d in enumerate(model.cpds[0].values):\n",
    "        if model.get_cpds(\"deg-malig\").name_to_no[\"deg-malig\"].get(row[6], None) != None:\n",
    "            result[j] *= float(model.get_cpds(\"deg-malig\").values[model.get_cpds(\"deg-malig\").name_to_no[\"deg-malig\"][row[6]]][j])\n",
    "        else: \n",
    "            result[j] *= 0.000001\n",
    "\n",
    "    predictions += [model.cpds[0].state_names[\"Class\"][result.index(max(result))]]\n",
    "print(\"Test accuracy: \", (predictions == dtest.Class.values).sum()/len(predictions))\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for row in dtrain.values:\n",
    "    result = [1, 1]\n",
    "    # test an example\n",
    "    for j, d in enumerate(model.cpds[0].values):\n",
    "        if model.get_cpds(\"deg-malig\").name_to_no[\"deg-malig\"].get(row[6], None) != None:\n",
    "            result[j] *= float(model.get_cpds(\"deg-malig\").values[model.get_cpds(\"deg-malig\").name_to_no[\"deg-malig\"][row[6]]][j])\n",
    "        else: \n",
    "            result[j] *= 0.000001\n",
    "\n",
    "    predictions += [model.cpds[0].state_names[\"Class\"][result.index(max(result))]]\n",
    "print(\"Train accuracy: \", (predictions == dtrain.Class.values).sum()/len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are very similar to those obtained with the Naive Bayes, which is very disapointing. Even thought we obtain these results, I do think this last approach is more capable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "**Naive Bayes:**\n",
    "\n",
    "|      |         K2         |        Bic        |      Accuracy      |\n",
    "|------|--------------------|-------------------|--------------------|\n",
    "| test | -639.5689356351188 | -683.769991489145 | 0.6379310344827587 |\n",
    "| train| -2327.1730910369565|-2377.1741867518376| 0.7368421052631579 |\n",
    "\n",
    "\n",
    "**Tree Structured Bayesian Classifier:**\n",
    "\n",
    "|      |         K2         |        Bic        |      Accuracy      |\n",
    "|------|--------------------|-------------------|--------------------|\n",
    "| test | -622.3407496923875 | -849.9164833843682| 0.7241379310344828 |\n",
    "| train| -2233.9267785881393|-2544.746122943686 | 0.7280701754385965 |\n",
    "\n",
    "\n",
    "**Bayesian Network:**\n",
    "\n",
    "|      |         K2         |        Bic        |      Accuracy      |\n",
    "|------|--------------------|-------------------|--------------------|\n",
    "| test | -479.60984292221355|-502.21788576729847| 0.6551724137931034 |\n",
    "| train| -1696.2692882744664|-1722.0429324104523| 0.7412280701754386 |\n",
    "\n",
    "\n",
    "From these results we can say that the model that performs the best is \n",
    "From these results it is interesting to mark that even though we get a better score for the BN accuracy but the other scores are significantly higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UwlIZRx7nRc"
   },
   "source": [
    "Finally learn a Bayesian network. **First learn the structure, and then the parameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzNfOBdPiINs"
   },
   "source": [
    "# **Learning Bayesian Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haooIWMtiSQP"
   },
   "source": [
    "We now want to learn a Bayesian network, given a set of sample data. Learning a Bayesian network can be split into two problems:\n",
    "\n",
    " **Structure learning**: Given a set of data samples, estimate a DAG that captures the dependencies between the variables.\n",
    "\n",
    "  **Parameter learning**: Given a set of data samples and a DAG that captures the dependencies between the variables, estimate the (conditional) probability distributions of the individual variables.\n",
    "\n",
    "\n",
    "Methods for doing this include:\n",
    "\n",
    "Structure learning for discrete, fully observed networks:\n",
    "    \n",
    "*    Score-based structure estimation (BIC/BDeu/K2 score; exhaustive search, hill climb/tabu search)\n",
    "*   Constraint-based structure estimation (PC)\n",
    "\n",
    "Parameter learning for discrete nodes:\n",
    "\n",
    "*   Maximum Likelihood Estimation\n",
    "*   Bayesian Estimation\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WglsabviF7O"
   },
   "source": [
    "**Structure Learning**\n",
    "\n",
    "You can use MLE or Bayesian estimation methods.\n",
    "\n",
    "MLE State counts\n",
    "\n",
    "To make sense of the given data, we can start by counting how often each state of the variable occurs. If the variable is dependent on parents, the counts are done conditionally on the parents states, i.e. for separately for each parent configuration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24DHWylrkm4q"
   },
   "source": [
    "**Bayesian Parameter Estimation**\n",
    "\n",
    "\n",
    "The Bayesian Parameter Estimator starts with already existing prior CPDs, that express our beliefs about the variables before the data was observed. Those \"priors\" are then updated, using the state counts from the observed data. \n",
    "\n",
    "One can think of the priors as consisting in pseudo state counts, that are added to the actual counts before normalization. Unless one wants to encode specific beliefs about the distributions of the variables, one commonly chooses uniform priors, i.e. ones that deem all states equiprobable.\n",
    "\n",
    "A very simple prior is the so-called K2 prior, which simply adds 1 to the count of every single state. A somewhat more sensible choice of prior is BDeu (Bayesian Dirichlet equivalent uniform prior). For BDeu we need to specify an equivalent sample size N and then the pseudo-counts are the equivalent of having observed N uniform samples of each variable (and each parent configuration). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaeRKm7qj-SS"
   },
   "source": [
    "**Maximum Likelihood Estimation**\n",
    "\n",
    "\n",
    "A natural estimate for the CPDs is to simply use the relative frequencies, with which the variable states have occured. \n",
    "\n",
    "This approach is Maximum Likelihood Estimation (MLE). According to MLE, we should fill the CPDs in such a way, that $P(\\text{data}|\\text{model})$ is maximal. This is achieved when using the relative frequencies.  pgmpy supports MLE as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0ey1LYUkeZs"
   },
   "source": [
    "\n",
    "mle.estimate_cpd(variable) computes the state counts and divides each cell by the (conditional) sample size. The mle.get_parameters()-method returns a list of CPDs for all variable of the model.\n",
    "\n",
    "The built-in fit()-method of BayesianModel provides more convenient access to parameter estimators:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_T9pPn62k0OQ"
   },
   "source": [
    "# **Structure Learning**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To learn model structure (a DAG) from a data set, there are two broad techniques:\n",
    "\n",
    "*   score-based structure learning\n",
    "*   constraint-based structure learning\n",
    "\n",
    "In this assignment focus on the score-based approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-N3IL_H2lF8m"
   },
   "source": [
    "# **Score-based Structure Learning**\n",
    "\n",
    "\n",
    "This approach construes model selection as an optimization task. It has two building blocks:\n",
    "\n",
    "A scoring function $s_D\\colon M \\to \\mathbb R$ that maps models to a numerical score, based on how well they fit to a given data set $D$.\n",
    "A search strategy to traverse the search space of possible models $M$ and select a model with optimal score.\n",
    "\n",
    "\n",
    "**Scoring functions**\n",
    "\n",
    "\n",
    "Commonly used scores to measure the fit between model and data are Bayesian Dirichlet scores such as BDeu or K2 and the Bayesian Information Criterion (BIC, also called MDL). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KEzCraWlUPg"
   },
   "source": [
    "\n",
    "**Search strategies**\n",
    "\n",
    "\n",
    "The search space of DAGs is super-exponential in the number of variables and the above scoring functions allow for local maxima. The first property makes exhaustive search intractable for all but very small networks, the second prohibits efficient local optimization algorithms to always find the optimal structure. Thus, identifiying the ideal structure is often not tractable. Despite these bad news, heuristic search strategies often yields good results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCj58MtblaLe"
   },
   "source": [
    "Heuristic search: HillClimbSearch implements a greedy local search that starts from the DAG start (default: disconnected DAG) and proceeds by iteratively performing single-edge manipulations that maximally increase the score. The search terminates once a local maximum is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rODmDHiku43"
   },
   "source": [
    "\n",
    "The estimated values in the CPDs are now more conservative. \n",
    "\n",
    "BayesianEstimator, too, can be used via the fit()-method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVqZN1ZVmdi7"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# **Discussion**\n",
    "\n",
    "Please critically compare the performance of the 3 different models."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
