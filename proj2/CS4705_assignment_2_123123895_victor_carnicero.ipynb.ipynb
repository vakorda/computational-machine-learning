{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gmprovan/CS4705/blob/main/Assignment_2_Learning_Bayesian_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .cont {\n",
    "        display: flex;\n",
    "        justify-content: center;\n",
    "        flex-direction: column;\n",
    "    }\n",
    "\n",
    "    .cont span {\n",
    "        margin:auto;\n",
    "    }\n",
    "    .name {\n",
    "        display:inline-block;\n",
    "        background-color: green;\n",
    "        padding: 10px;\n",
    "        border-radius: 20px;\n",
    "        margin:auto;\n",
    "    }\n",
    "    .class {\n",
    "        display:inline-block;\n",
    "        background-color: purple;\n",
    "        padding: 10px;\n",
    "        border-radius: 20px;\n",
    "        margin:auto;\n",
    "    }\n",
    "    .assignment {\n",
    "        display:inline-block;\n",
    "        background-color: white;\n",
    "        color: black;\n",
    "        font-weight: bolder;\n",
    "        font-size: 60px;\n",
    "        padding: 10px;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"cont\">\n",
    "    <span><span class=\"assignment\">Assignment 2 -</span><span class=\"assignment\">Probabilistic Model Learning</span></span>\n",
    "    <br>\n",
    "    <span><span class=\"name\">Víctor Carnicero Príncipe</span><span class=\"name\">123123895</span></span>\n",
    "    <br>\n",
    "    <span><span class=\"class\">COMPUTATIONAL MACHINE LEARNING</span><span class=\"class\">CS4705</span></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Víctor Carnicero Príncipe | 123123895 || COMPUTATIONAL MACHINE LEARNING | CS4705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pgmpy.estimators import K2Score, BicScore\n",
    "from pgmpy.models import BayesianNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('./data/breast-cancer.data',\n",
    "                   names=['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change ? to NaN\n",
    "data[data == \"?\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove problematic data points\n",
    "data = data[data[\"age\"] != \"20-29\"]\n",
    "data = data[data[\"inv-nodes\"] != \"24-25\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate train and test data\n",
    "dtrain, dtest = train_test_split(data.fillna(data.mode().iloc[0]), test_size=0.2, random_state=1)\n",
    "\n",
    "predictions = {\"nb\": {}, \"t\": {}, \"bn\": {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE3ITHto7e5v"
   },
   "source": [
    "## Naive Bayes model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the naive bayes I use the maximum likelihood estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "NhVCpGKUM3q3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+----------+\n",
      "| Class(no-recurrence-events) | 0.688596 |\n",
      "+-----------------------------+----------+\n",
      "| Class(recurrence-events)    | 0.311404 |\n",
      "+-----------------------------+----------+\n",
      "test:\n",
      " K2: -613.0600117480078 | Bic: -655.5336891240779\n",
      "train:\n",
      " K2: -2331.4476306110555 | Bic: -2380.6982978994574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 2044.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:\t 0.7017543859649122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [00:00<00:00, 2100.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:\t 0.7719298245614035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.models.NaiveBayes import NaiveBayes\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "\n",
    "# create the structure manually to create model_struct\n",
    "model_nb = BayesianNetwork(NaiveBayes(data.columns[1:], data.columns[0]).edges())\n",
    "#model_nb = NaiveBayes(data.columns[1:], data.columns[0])\n",
    "\n",
    "model_nb.fit(dtrain, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "print(model_nb.get_cpds('Class'))\n",
    "\n",
    "# print the score\n",
    "print(\"test:\\n\", \"K2:\", K2Score(dtest).score(model_nb), \"| Bic:\", BicScore(dtest).score(model_nb))\n",
    "print(\"train:\\n\", \"K2:\", K2Score(dtrain).score(model_nb), \"| Bic:\", BicScore(dtrain).score(model_nb))\n",
    "print(\"Test accuracy:\\t\", test:= (model_nb.predict(dtest[data.columns[1:]]).values == dtest[[\"Class\"]].values).sum()/len(dtest))\n",
    "print(\"Train accuracy:\\t\", train:= (model_nb.predict(dtrain[data.columns[1:]]).values == dtrain[[\"Class\"]].values).sum()/len(dtrain))\n",
    "predictions[\"nb\"][\"test\"] = test\n",
    "predictions[\"nb\"][\"train\"] = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores obtained are good enough, we do seem to have a difference between test and training data, which probably indicates some overfitting to the train data.\n",
    "\n",
    "The model seems to work with somewhat acceptable performance, we do see that there is a `7%` difference between train and test accuracy. We can probably get a better model using more complex networks, so I expect the following models to perform better than this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0ADjdH07jFb"
   },
   "source": [
    "## Tree-structured model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "2W52lkXeMqqr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building tree: 100%|██████████| 45/45.0 [00:00<00:00, 4085.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import TreeSearch\n",
    "\n",
    "# learn graph structure using TreeSearch and Chow-Liu algorithm\n",
    "est = TreeSearch(dtrain, root_node=\"Class\")\n",
    "dag = est.estimate(estimator_type=\"chow-liu\")\n",
    "\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "\n",
    "# there are many choices of parametrization, here is one example\n",
    "model_t = BayesianNetwork(dag.edges())\n",
    "model_t.fit(data, estimator=BayesianEstimator, prior_type=\"dirichlet\", pseudo_counts=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:\n",
      " K2: -602.3680239908574 | Bic: -800.6569955287398\n",
      "train:\n",
      " K2: -2236.4249916106974 | Bic: -2546.7317696896125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 5176.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  0.7543859649122807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [00:00<00:00, 2671.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  0.7105263157894737\n"
     ]
    }
   ],
   "source": [
    "# print the score\n",
    "print(\"test:\\n\", \"K2:\", K2Score(dtest).score(model_t), \"| Bic:\", BicScore(dtest).score(model_t))\n",
    "print(\"train:\\n\", \"K2:\", K2Score(dtrain).score(model_t), \"| Bic:\", BicScore(dtrain).score(model_t))\n",
    "# print the accuracy\n",
    "print(\"test: \", test:=((model_t.predict(dtest[data.columns[1:]]).values == dtest[[data.columns[0]]].reset_index(drop=True).values).sum() / len(dtest)))\n",
    "print(\"train: \", train:=((model_t.predict(dtrain[data.columns[1:]]).values == dtrain[[data.columns[0]]].reset_index(drop=True).values).sum() / len(dtrain)))\n",
    "predictions[\"t\"][\"test\"] = test\n",
    "predictions[\"t\"][\"train\"] = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the scores of the two models, we can see that the Naive Bayes model has a better score for all the values. This model seems to be good, specially since we get a very similar accuracy for the train and test set, which seems to indicate that the model is a good generalization and does not overfit the training data.\n",
    "\n",
    "The accuracy obtained is good enough, it might be possible to obtain a better score using the following method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionallly I decide to show the predictive accuracy for the other variables, since it is a Bayesian Model, it is easy to predict the value for any missing variable from all the others using the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 1939.77it/s]\n",
      "100%|██████████| 215/215 [00:00<00:00, 1077.35it/s]\n",
      "100%|██████████| 56/56 [00:00<00:00, 2286.19it/s]\n",
      "100%|██████████| 206/206 [00:00<00:00, 1051.93it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 2296.42it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1182.11it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 2157.82it/s]\n",
      "100%|██████████| 183/183 [00:00<00:00, 938.91it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 1859.83it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 800.52it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 1413.58it/s]\n",
      "100%|██████████| 217/217 [00:00<00:00, 1110.44it/s]\n",
      "100%|██████████| 56/56 [00:00<00:00, 2021.54it/s]\n",
      "100%|██████████| 211/211 [00:00<00:00, 1233.86it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 1419.01it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1136.89it/s]\n",
      "100%|██████████| 56/56 [00:00<00:00, 1740.19it/s]\n",
      "100%|██████████| 198/198 [00:00<00:00, 967.86it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 1708.35it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1150.07it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = {}\n",
    "for col in data:\n",
    "    preds[col] = [((model_t.predict(dtest.drop(columns=[col])).values == dtest[[col]].reset_index(drop=True).values).sum() / len(dtest)),\n",
    "                  ((model_t.predict(dtrain.drop(columns=[col])).values == dtrain[[col]].reset_index(drop=True).values).sum() / len(dtrain))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test | train\n",
      "Class [0.7543859649122807, 0.7105263157894737]\n",
      "age [0.5263157894736842, 0.5131578947368421]\n",
      "menopause [0.8947368421052632, 0.8114035087719298]\n",
      "tumor-size [0.24561403508771928, 0.2850877192982456]\n",
      "inv-nodes [0.8070175438596491, 0.8070175438596491]\n",
      "node-caps [0.8421052631578947, 0.8859649122807017]\n",
      "deg-malig [0.543859649122807, 0.5131578947368421]\n",
      "breast [0.6491228070175439, 0.6754385964912281]\n",
      "breast-quad [0.45614035087719296, 0.5263157894736842]\n",
      "irradiat [0.8245614035087719, 0.7850877192982456]\n"
     ]
    }
   ],
   "source": [
    "print(\"test\", \"|\", \"train\")\n",
    "for col in preds:\n",
    "    print(col, preds[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some variables that are very bad in this case, like `tumor-size`, `deg-malign`, `breast-quad` and `age`, whereas others are even better than the `Class` variable, it is very difficult to obtain a good score for all the variables but in the same way it is good to know we have some way to predict other variables if we have any problems.\n",
    "\n",
    "This can also help us understand which variables are actually important to the model, we could continue trying some different subset of the variables to try to obtain a better model.\n",
    "\n",
    "The following approach takes this into account and in the structure building it may be possible to extract deeper relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Network model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn Structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model creation I use hill climbing, because the whole searching space is very big this approach will get to a local minimum. First I start with a graph with no edges, and then I explore by starting with the previously used structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('menopause', 'age'), ('node-caps', 'inv-nodes'), ('node-caps', 'deg-malig'), ('deg-malig', 'Class'), ('breast', 'breast-quad'), ('irradiat', 'node-caps')]\n",
      "test:\n",
      " K2: -455.10833332994423 | Bic: -479.57666873007065\n",
      "train:\n",
      " K2: -1701.8622890331535 | Bic: -1729.0220587763633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:00<00:00, 4727.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 0.7543859649122807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:00<00:00, 2592.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.7105263157894737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import HillClimbSearch, BicScore, K2Score, BDeuScore, ExpectationMaximization\n",
    "\n",
    "est = HillClimbSearch(dtrain)\n",
    "\n",
    "# estimate the DAG\n",
    "model_bn = BayesianNetwork(est.estimate(scoring_method=BicScore(dtrain), show_progress=False).edges()) \n",
    "print(model_bn.edges())\n",
    "\n",
    "model_bn.fit(dtrain, estimator=BayesianEstimator, prior_type=\"dirichlet\", pseudo_counts=0.1)\n",
    "\n",
    "# print the score\n",
    "print(\"test:\\n\", \"K2:\", K2Score(dtest).score(model_bn), \"| Bic:\", BicScore(dtest).score(model_bn))\n",
    "print(\"train:\\n\", \"K2:\", K2Score(dtrain).score(model_bn), \"| Bic:\", BicScore(dtrain).score(model_bn))\n",
    "\n",
    "print(\"test:\", test:=(model_bn.predict(dtest[list(model_bn.nodes()._nodes.keys())].drop(columns=[\"Class\"])).values == dtest[[data.columns[0]]].reset_index(drop=True).values).sum() / len(dtest))\n",
    "print(\"train:\", train:=(model_bn.predict(dtrain[list(model_bn.nodes()._nodes.keys())].drop(columns=[\"Class\"])).values == dtrain[[data.columns[0]]].reset_index(drop=True).values).sum() / len(dtrain))\n",
    "predictions[\"bn\"][\"test\"] = test\n",
    "predictions[\"bn\"][\"train\"] = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this classifier we obtain the same accuracy as the previous one, but we do seem to obtain a better `K2` value and a worse `Bic`.\n",
    "\n",
    "Like the one before, both scores seem to be pretty near, so there does not seem to be overfitting, and in the same way we obtain a good enough accuracy of around `75%`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start by using a naive bayes structure and then trying to obtain a good bayesian network using hill climbing and the BDeu score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Class', 'inv-nodes'), ('Class', 'deg-malig'), ('inv-nodes', 'node-caps'), ('inv-nodes', 'irradiat'), ('node-caps', 'deg-malig'), ('breast', 'node-caps'), ('breast-quad', 'breast'), ('age', 'menopause')]\n",
      "test:\n",
      " K2: -451.67936080665385 | Bic: -490.41432567076083\n",
      "train:\n",
      " K2: -1703.3800204008173 | Bic: -1759.744048841769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:00<00:00, 1218.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8070175438596491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:00<00:00, 980.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import HillClimbSearch, BicScore, K2Score, BDeuScore, ExpectationMaximization\n",
    "\n",
    "model_bn2 = BayesianNetwork(est.estimate(start_dag=BayesianNetwork(NaiveBayes(data.columns[1:], data.columns[0]).edges()), scoring_method=BDeuScore(dtrain), show_progress=False).edges()) \n",
    "print(model_bn2.edges())\n",
    "\n",
    "model_bn2.fit(dtrain, estimator=BayesianEstimator, prior_type=\"dirichlet\", pseudo_counts=0.1)\n",
    "\n",
    "# print the score\n",
    "print(\"test:\\n\", \"K2:\", K2Score(dtest).score(model_bn2), \"| Bic:\", BicScore(dtest).score(model_bn2))\n",
    "print(\"train:\\n\", \"K2:\", K2Score(dtrain).score(model_bn2), \"| Bic:\", BicScore(dtrain).score(model_bn2))\n",
    "\n",
    "print((model_bn2.predict(dtest[list(model_bn2.nodes()._nodes.keys())].drop(columns=[\"Class\"])).values == dtest[[data.columns[0]]].reset_index(drop=True).values).sum() / len(dtest))\n",
    "print((model_bn2.predict(dtrain[list(model_bn2.nodes()._nodes.keys())].drop(columns=[\"Class\"])).values == dtrain[[data.columns[0]]].reset_index(drop=True).values).sum() / len(dtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not know why this model performs this well for the test data, it is probably due to an eventuallity since there is not much data. The model performs with a `7%` less performance for the training data than for the testing, which is interesting. This is why I prefer the model before this one, since both scores are more similar.\n",
    "\n",
    "Now trying to start with the tree-structured bayesian classifier and K2 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Class', 'deg-malig'), ('Class', 'tumor-size'), ('Class', 'age'), ('Class', 'irradiat'), ('deg-malig', 'tumor-size'), ('deg-malig', 'age'), ('age', 'tumor-size'), ('irradiat', 'tumor-size'), ('irradiat', 'deg-malig'), ('irradiat', 'age'), ('inv-nodes', 'node-caps'), ('inv-nodes', 'tumor-size'), ('inv-nodes', 'irradiat'), ('inv-nodes', 'deg-malig'), ('inv-nodes', 'age'), ('node-caps', 'tumor-size'), ('node-caps', 'deg-malig'), ('node-caps', 'Class'), ('node-caps', 'age'), ('node-caps', 'irradiat'), ('breast-quad', 'breast'), ('breast-quad', 'tumor-size'), ('breast-quad', 'deg-malig'), ('breast-quad', 'age'), ('breast', 'tumor-size'), ('breast', 'deg-malig'), ('breast', 'age'), ('breast', 'node-caps'), ('breast', 'Class'), ('menopause', 'tumor-size'), ('menopause', 'deg-malig'), ('menopause', 'age')]\n",
      "test:\n",
      " K2: 241242.85360748318 | Bic: -361847.7775965159\n",
      "train:\n",
      " K2: 392162.6668773573 | Bic: -749493.0765869628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 91.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6140350877192983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [00:03<00:00, 55.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import HillClimbSearch, BicScore, K2Score, BDeuScore, ExpectationMaximization\n",
    "\n",
    "model_bn3 = BayesianNetwork(est.estimate(start_dag=BayesianNetwork(dag.edges()), scoring_method=K2Score(dtrain), show_progress=False).edges()) \n",
    "print(model_bn3.edges())\n",
    "\n",
    "model_bn3.fit(dtrain, estimator=BayesianEstimator, prior_type=\"dirichlet\", pseudo_counts=0.1)\n",
    "\n",
    "# print the score\n",
    "print(\"test:\\n\", \"K2:\", K2Score(dtest).score(model_bn3), \"| Bic:\", BicScore(dtest).score(model_bn3))\n",
    "print(\"train:\\n\", \"K2:\", K2Score(dtrain).score(model_bn3), \"| Bic:\", BicScore(dtrain).score(model_bn3))\n",
    "\n",
    "print((model_bn3.predict(dtest[list(model_bn3.nodes()._nodes.keys())].drop(columns=[\"Class\"])).values == dtest[[data.columns[0]]].reset_index(drop=True).values).sum() / len(dtest))\n",
    "print((model_bn3.predict(dtrain[list(model_bn3.nodes()._nodes.keys())].drop(columns=[\"Class\"])).values == dtrain[[data.columns[0]]].reset_index(drop=True).values).sum() / len(dtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can clearly see overfitting to the training data, it performs extremely well for it `98%`, and bad for the test data `62%`. This is why I will not consider this model, as it is pretty bad, I prefer the first model generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "**Naive Bayes:**\n",
    "\n",
    "|      |         K2         |        Bic        |      Accuracy      |\n",
    "|------|--------------------|-------------------|--------------------|\n",
    "| test | -613.0600117480078 | -655.5336891240779| 0.7017543859649122 |\n",
    "| train| -2331.4476306110555|-2380.6982978994574| 0.7719298245614035 |\n",
    "\n",
    "**Tree Structured Bayesian Classifier:**\n",
    "\n",
    "|      |         K2         |        Bic        |      Accuracy      |\n",
    "|------|--------------------|-------------------|--------------------|\n",
    "| test | -602.3680239908574 | -800.6569955287398| 0.7543859649122807 |\n",
    "| train| -2236.4249916106974|-2546.7317696896125| 0.7105263157894737 |\n",
    "\n",
    "**Bayesian Network:**\n",
    "\n",
    "|      |         K2         |        Bic        |      Accuracy      |\n",
    "|------|--------------------|-------------------|--------------------|\n",
    "| test | -455.1083333299442 |-479.57666873007065| 0.7543859649122807 |\n",
    "| train| -1701.8622890331535|-1729.0220587763633| 0.7105263157894737 |\n",
    "\n",
    "\n",
    "From these results we can say that the model that performs the best are the Tree-structured and bayesian network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to do McNemar's Test (5x2 Cross validation) but after trying a lot I was not able to get it to work.\n",
    "\n",
    "So I decided to compute a p-value based on an f-statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value for nb vs t: 0.2749369765547082\n",
      "Hyphothesis test for nb vs t: Accept\n",
      "p-value for nb vs bn: 0.2749369765547082\n",
      "Hyphothesis test for nb vs bn: Accept\n",
      "p-value for t vs bn: 0.500000000000054\n",
      "Hyphothesis test for t vs bn: Accept\n"
     ]
    }
   ],
   "source": [
    "# import stats\n",
    "from scipy import stats\n",
    "\n",
    "def get_pvalue(model1, model2, data):\n",
    "    f_value = predictions[model1][\"test\"] / predictions[model2][\"test\"]\n",
    "    df = data.shape[0] - data.shape[1] - 1\n",
    "    p_value = stats.f.cdf(f_value, df, df)\n",
    "    return p_value\n",
    "\n",
    "def accept_or_reject(p_value):\n",
    "    if p_value > 0.1 or p_value < 0.9:\n",
    "        return \"Accept\"\n",
    "    else:\n",
    "        return \"Reject\"\n",
    "\n",
    "print(\"p-value for nb vs t:\", p:=get_pvalue(\"nb\", \"t\", data))\n",
    "print(\"Hyphothesis test for nb vs t:\", accept_or_reject(p))\n",
    "print(\"p-value for nb vs bn:\", p:=get_pvalue(\"nb\", \"bn\", data))\n",
    "print(\"Hyphothesis test for nb vs bn:\", accept_or_reject(p))\n",
    "print(\"p-value for t vs bn:\", p:=get_pvalue(\"t\", \"bn\", data))\n",
    "print(\"Hyphothesis test for t vs bn:\", accept_or_reject(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we accept all the null hypotheses that means that there is no evidence to say that this models perform significantly different.\n",
    "\n",
    "For the tree structured and bayesian network, since I obtain the same accuracy results for both of them I cannot really be\n",
    "\n",
    "sure of which is best, and as expected we obtain a p-value of 0.5, and I am not too sure of how to interpret the K2 and Bic\n",
    "\n",
    "score. The aim is to obtain a higher K2 score, and a lower Bic, since they both change similarly between the two models I\n",
    "\n",
    "cannot really tell which is better."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
